import numpy as np

# Environment setup
num_states = 16  # Number of states in the grid world
num_actions = 4  # Number of possible actions (up, down, left, right)
goal_state = 15  # Goal state in the grid
obstacle_states = [5, 7, 11]  # Obstacle states in the grid

# Q-learning parameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Exploration-exploitation trade-off
num_episodes = 1000
max_steps_per_episode = 100  # Maximum number of steps per episode

# Q-table initialization
Q = np.zeros((num_states, num_actions))

# Q-learning algorithm
for episode in range(num_episodes):
    state = 0  # Starting state
    done = False
    steps = 0

    while not done and steps < max_steps_per_episode:
        # Exploration-exploitation trade-off
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.randint(num_actions)  # Exploration
        else:
            action = np.argmax(Q[state, :])  # Exploitation

        # Take action and observe the next state and reward
        if state in obstacle_states:
            next_state = state  # Stay in the same state if an obstacle is encountered
            reward = -1  # Negative reward for hitting an obstacle
        else:
            if action == 0:
                next_state = state - 4 if state >= 4 else state  # Move up
            elif action == 1:
                next_state = state + 4 if state < 12 else state  # Move down
            elif action == 2:
                next_state = state - 1 if state % 4 != 0 else state  # Move left
            else:
                next_state = state + 1 if (state + 1) % 4 != 0 else state  # Move right

            if next_state == goal_state:
                reward = 1  # Positive reward for reaching the goal
                done = True
            else:
                reward = 0  # No reward for regular moves

        # Update Q-value using the Bellman equation
        Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]))

        state = next_state
        steps += 1

# Print the learned Q-table
print("Learned Q-table:")
print(Q)

